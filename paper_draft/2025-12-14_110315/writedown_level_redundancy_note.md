# Note on apparent redundancy across levels

At first glance, several task definitions look algebraically similar after a log transform. For lognormal features, define $z_i = \log x_i$ and ignore stabilizers. Then a Level 1 ratio is approximately $u \approx z_0 - z_1$, and a Level 1 product is approximately $u \approx z_0 + z_1$. Some higher levels are also linear or bilinear forms in $z$. For example, a Level 3 ratio difference is still a linear form, and Level 4 interactions are products of two linear forms. This does not imply that the levels are redundant as learning problems, because the benchmark varies the joint distribution of the raw inputs and evaluates finite-capacity models trained on raw $x$ rather than on $z$.

Two mechanisms make these tasks behave differently in practice. First, the joint distribution breaks the symmetries that would make “sum” and “difference” directions equivalent. With correlated inputs, $z_0 + z_1$ and $z_0 - z_1$ have different variance and tail behavior. In a simple Gaussian approximation with $\mathrm{Corr}(z_0,z_1)=\rho$, one has $\mathrm{Var}(z_0+z_1)=2(1+\rho)$ and $\mathrm{Var}(z_0-z_1)=2(1-\rho)$. With fixed label noise and fixed slope in the link, this changes Bayes separability and sample complexity even when the functional form is “the same up to a rotation”. Second, the learner does not observe $z$. It sees many raw, positive, heavy-tailed variables with distractors and correlation structure. Axis-aligned trees can exploit magnitude and tail shortcuts that depend on the raw distribution. Those shortcuts can differ sharply between tasks that are algebraically related in log-space, and they can collapse under shifts that preserve the intended coordinate but change raw magnitudes.

For these reasons, it is reasonable to keep the current level definitions, but the paper should describe them as a set of related motifs rather than a set of strictly non-overlapping function classes. Apparent algebraic similarity is a feature of the design, because it allows the benchmark to isolate the effect of nuisance distributions and shifts. When two tasks share the same “primitive” in log-space but show different $\Delta$ under a correlation or shift regime, that difference is itself evidence about shortcut learning and invariance failure rather than an artifact of redundant definitions.
